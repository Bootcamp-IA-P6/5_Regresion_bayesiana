{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a672b3",
   "metadata": {},
   "source": [
    "# Modelo 4: Regresión Bayesiana para Predicción de Revenue\n",
    "\n",
    "Este modelo predice el `total_revenue` usando múltiples features del dataset de Amazon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a822320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f3877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y explorar datos\n",
    "df = pd.read_csv('dataset/amazon_sales_dataset.csv')\n",
    "print(f\"Dimensiones del dataset: {df.shape}\")\n",
    "print(f\"\\nPrimeras filas:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información básica del dataset\n",
    "print(\"Información del dataset:\")\n",
    "print(df.info())\n",
    "print(f\"\\nValores nulos:\\n{df.isnull().sum()}\")\n",
    "print(f\"\\nEstadísticas descriptivas:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d099dc",
   "metadata": {},
   "source": [
    "## Análisis Exploratorio de Datos (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c9e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA - Distribución de la variable objetivo\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(df['total_revenue'], bins=50, alpha=0.7)\n",
    "plt.title('Distribución de Total Revenue')\n",
    "plt.xlabel('Total Revenue')\n",
    "plt.ylabel('Frecuencia')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.boxplot(df['total_revenue'])\n",
    "plt.title('Boxplot de Total Revenue')\n",
    "plt.ylabel('Total Revenue')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "from scipy import stats\n",
    "stats.probplot(df['total_revenue'], dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901d647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlaciones\n",
    "numeric_cols = ['price', 'discount_percent', 'quantity_sold', 'rating', 'review_count', \n",
    "                'discounted_price', 'total_revenue']\n",
    "correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Matriz de Correlación')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Correlación con total_revenue:\\n{correlation_matrix['total_revenue'].sort_values(ascending=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657d6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots con variables más correlacionadas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Price vs Total Revenue\n",
    "axes[0,0].scatter(df['price'], df['total_revenue'], alpha=0.5)\n",
    "axes[0,0].set_xlabel('Price')\n",
    "axes[0,0].set_ylabel('Total Revenue')\n",
    "axes[0,0].set_title('Price vs Total Revenue')\n",
    "\n",
    "# Quantity vs Total Revenue\n",
    "axes[0,1].scatter(df['quantity_sold'], df['total_revenue'], alpha=0.5)\n",
    "axes[0,1].set_xlabel('Quantity Sold')\n",
    "axes[0,1].set_ylabel('Total Revenue')\n",
    "axes[0,1].set_title('Quantity Sold vs Total Revenue')\n",
    "\n",
    "# Discounted Price vs Total Revenue\n",
    "axes[1,0].scatter(df['discounted_price'], df['total_revenue'], alpha=0.5)\n",
    "axes[1,0].set_xlabel('Discounted Price')\n",
    "axes[1,0].set_ylabel('Total Revenue')\n",
    "axes[1,0].set_title('Discounted Price vs Total Revenue')\n",
    "\n",
    "# Rating vs Total Revenue\n",
    "axes[1,1].scatter(df['rating'], df['total_revenue'], alpha=0.5)\n",
    "axes[1,1].set_xlabel('Rating')\n",
    "axes[1,1].set_ylabel('Total Revenue')\n",
    "axes[1,1].set_title('Rating vs Total Revenue')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0467c836",
   "metadata": {},
   "source": [
    "## Preparación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4063122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar features para el modelo\n",
    "# Basándonos en las correlaciones, usaremos las variables más relevantes\n",
    "features = ['discounted_price', 'quantity_sold', 'rating']\n",
    "target = 'total_revenue'\n",
    "\n",
    "X = df[features].copy()\n",
    "y = df[target].copy()\n",
    "\n",
    "print(f\"Features seleccionadas: {features}\")\n",
    "print(f\"Dimensiones X: {X.shape}\")\n",
    "print(f\"Dimensiones y: {y.shape}\")\n",
    "\n",
    "# Verificar datos faltantes\n",
    "print(f\"\\nDatos faltantes en X:\\n{X.isnull().sum()}\")\n",
    "print(f\"\\nDatos faltantes en y: {y.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943eac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# División train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Tamaño conjunto entrenamiento: {X_train.shape[0]}\")\n",
    "print(f\"Tamaño conjunto prueba: {X_test.shape[0]}\")\n",
    "\n",
    "# Normalización de features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nEstadísticas después de normalización (train):\")\n",
    "print(f\"Media: {X_train_scaled.mean(axis=0)}\")\n",
    "print(f\"Std: {X_train_scaled.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fa3d7f",
   "metadata": {},
   "source": [
    "## Modelo Bayesiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886f9ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelo bayesiano\n",
    "def create_bayesian_model(X, y):\n",
    "    with pm.Model() as model:\n",
    "        # Priors para los coeficientes\n",
    "        intercept = pm.Normal('intercept', mu=0, sigma=10)\n",
    "        coeffs = pm.Normal('coeffs', mu=0, sigma=10, shape=X.shape[1])\n",
    "        \n",
    "        # Prior para la desviación estándar del error\n",
    "        sigma = pm.HalfNormal('sigma', sigma=10)\n",
    "        \n",
    "        # Media del modelo\n",
    "        mu = intercept + pm.math.dot(X, coeffs)\n",
    "        \n",
    "        # Likelihood\n",
    "        likelihood = pm.Normal('y', mu=mu, sigma=sigma, observed=y)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear y entrenar modelo\n",
    "print(\"Creando modelo bayesiano...\")\n",
    "model = create_bayesian_model(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Entrenando modelo...\")\n",
    "with model:\n",
    "    # Sampling\n",
    "    trace = pm.sample(2000, tune=1000, random_seed=42, chains=2)\n",
    "\n",
    "print(\"Modelo entrenado exitosamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca694d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnósticos del modelo\n",
    "print(\"Resumen de parámetros:\")\n",
    "print(az.summary(trace))\n",
    "\n",
    "# Visualizar trazas\n",
    "az.plot_trace(trace)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# R-hat (debe estar cerca de 1)\n",
    "rhat = az.rhat(trace)\n",
    "print(f\"\\nR-hat valores (deben estar cerca de 1):\")\n",
    "print(rhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b45b8bd",
   "metadata": {},
   "source": [
    "## Predicciones y Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c9a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para hacer predicciones\n",
    "def make_predictions(model, trace, X_new):\n",
    "    with model:\n",
    "        pm.set_data({'X': X_new})\n",
    "        posterior_predictive = pm.sample_posterior_predictive(trace, random_seed=42)\n",
    "    \n",
    "    predictions = posterior_predictive.posterior_predictive['y'].values\n",
    "    pred_mean = predictions.mean(axis=(0, 1))\n",
    "    pred_std = predictions.std(axis=(0, 1))\n",
    "    \n",
    "    return pred_mean, pred_std\n",
    "\n",
    "# Hacer predicciones para train y test\n",
    "print(\"Realizando predicciones...\")\n",
    "\n",
    "# Para entrenamient\n",
    "with model:\n",
    "    pm.set_data({'X': X_train_scaled, 'y': y_train})\n",
    "    ppc_train = pm.sample_posterior_predictive(trace, random_seed=42)\n",
    "\n",
    "# Para test\n",
    "with model:\n",
    "    pm.set_data({'X': X_test_scaled, 'y': y_test})\n",
    "    ppc_test = pm.sample_posterior_predictive(trace, random_seed=42)\n",
    "\n",
    "# Extraer predicciones\n",
    "y_pred_train = ppc_train.posterior_predictive['y'].mean(dim=['chain', 'draw']).values\n",
    "y_pred_test = ppc_test.posterior_predictive['y'].mean(dim=['chain', 'draw']).values\n",
    "\n",
    "print(\"Predicciones completadas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b1fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular métricas\n",
    "def calculate_metrics(y_true, y_pred, dataset_name):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\nMétricas {dataset_name}:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    \n",
    "    return {'RMSE': rmse, 'MAE': mae, 'R2': r2}\n",
    "\n",
    "# Métricas de entrenamiento y prueba\n",
    "metrics_train = calculate_metrics(y_train, y_pred_train, \"Entrenamiento\")\n",
    "metrics_test = calculate_metrics(y_test, y_pred_test, \"Prueba\")\n",
    "\n",
    "# Verificar overfitting\n",
    "overfitting_rmse = abs(metrics_train['RMSE'] - metrics_test['RMSE']) / metrics_train['RMSE'] * 100\n",
    "overfitting_r2 = abs(metrics_train['R2'] - metrics_test['R2']) / metrics_train['R2'] * 100\n",
    "\n",
    "print(f\"\\nAnálisis de Overfitting:\")\n",
    "print(f\"Diferencia RMSE: {overfitting_rmse:.2f}%\")\n",
    "print(f\"Diferencia R²: {overfitting_r2:.2f}%\")\n",
    "\n",
    "if overfitting_rmse < 5 and overfitting_r2 < 5:\n",
    "    print(\"✅ Overfitting bajo: < 5%\")\n",
    "else:\n",
    "    print(\"⚠️ Posible overfitting detectado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61194670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones de evaluación\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Predicciones vs Reales (Train)\n",
    "axes[0,0].scatter(y_train, y_pred_train, alpha=0.5)\n",
    "axes[0,0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "axes[0,0].set_xlabel('Valores Reales')\n",
    "axes[0,0].set_ylabel('Predicciones')\n",
    "axes[0,0].set_title(f'Entrenamiento: R² = {metrics_train[\"R2\"]:.4f}')\n",
    "\n",
    "# Predicciones vs Reales (Test)\n",
    "axes[0,1].scatter(y_test, y_pred_test, alpha=0.5)\n",
    "axes[0,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0,1].set_xlabel('Valores Reales')\n",
    "axes[0,1].set_ylabel('Predicciones')\n",
    "axes[0,1].set_title(f'Prueba: R² = {metrics_test[\"R2\"]:.4f}')\n",
    "\n",
    "# Residuos (Train)\n",
    "residuals_train = y_train - y_pred_train\n",
    "axes[1,0].scatter(y_pred_train, residuals_train, alpha=0.5)\n",
    "axes[1,0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1,0].set_xlabel('Predicciones')\n",
    "axes[1,0].set_ylabel('Residuos')\n",
    "axes[1,0].set_title('Residuos - Entrenamiento')\n",
    "\n",
    "# Residuos (Test)\n",
    "residuals_test = y_test - y_pred_test\n",
    "axes[1,1].scatter(y_pred_test, residuals_test, alpha=0.5)\n",
    "axes[1,1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1,1].set_xlabel('Predicciones')\n",
    "axes[1,1].set_ylabel('Residuos')\n",
    "axes[1,1].set_title('Residuos - Prueba')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fd6e8f",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f62819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer coeficientes posteriores\n",
    "coefficients = trace.posterior['coeffs'].values\n",
    "coeff_means = coefficients.mean(axis=(0, 1))\n",
    "coeff_stds = coefficients.std(axis=(0, 1))\n",
    "\n",
    "# Crear DataFrame con importancia de features\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient_Mean': coeff_means,\n",
    "    'Coefficient_Std': coeff_stds,\n",
    "    'Absolute_Importance': np.abs(coeff_means)\n",
    "})\n",
    "\n",
    "feature_importance = feature_importance.sort_values('Absolute_Importance', ascending=False)\n",
    "print(\"Importancia de Features:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Visualizar importancia\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Coefficient_Mean'])\n",
    "plt.xlabel('Coeficiente Promedio')\n",
    "plt.title('Importancia de Features (Coeficientes del Modelo Bayesiano)')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb64731",
   "metadata": {},
   "source": [
    "## Guardar Modelo y Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78728a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar artefactos del modelo\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Guardar scaler\n",
    "joblib.dump(scaler, 'modelo_4_scaler.pkl')\n",
    "\n",
    "# Guardar trace (objeto PyMC)\n",
    "with open('modelo_4_trace.pkl', 'wb') as f:\n",
    "    pickle.dump(trace, f)\n",
    "\n",
    "# Guardar métricas\n",
    "results = {\n",
    "    'features': features,\n",
    "    'metrics_train': metrics_train,\n",
    "    'metrics_test': metrics_test,\n",
    "    'feature_importance': feature_importance.to_dict(),\n",
    "    'overfitting_rmse': overfitting_rmse,\n",
    "    'overfitting_r2': overfitting_r2\n",
    "}\n",
    "\n",
    "with open('modelo_4_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "print(\"Modelo y resultados guardados exitosamente!\")\n",
    "print(\"Archivos generados:\")\n",
    "print(\"- modelo_4_scaler.pkl\")\n",
    "print(\"- modelo_4_trace.pkl\")\n",
    "print(\"- modelo_4_results.pkl\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
