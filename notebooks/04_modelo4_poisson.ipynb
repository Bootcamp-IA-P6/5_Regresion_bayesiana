{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dff717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTENSOR_FLAGS\"] = \"cxx=\"\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURACION GLOBAL — Modelo 4: Regresion de Poisson Bayesiana\n",
    "# ============================================================\n",
    "RANDOM_SEED = 42\n",
    "TEST_SIZE = 0.20\n",
    "\n",
    "# MCMC (NUTS)\n",
    "TUNE_SAMPLES = 1000\n",
    "DRAW_SAMPLES = 1000\n",
    "TARGET_ACCEPT = 0.90\n",
    "\n",
    "# Umbral overfitting (diferencia relativa MAE train-val)\n",
    "OVERFIT_THRESHOLD = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fda7cd3",
   "metadata": {},
   "source": [
    "# 04 — Modelo 4: Regresion de Poisson Bayesiana\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Modelar **quantity_sold** (conteo de unidades vendidas) usando una distribucion de **Poisson Bayesiana**.\n",
    "\n",
    "### Predictores\n",
    "\n",
    "- `discount_percent`\n",
    "- `rating`\n",
    "- `is_weekend`\n",
    "\n",
    "> Pipeline: EDA rapido, preparacion, modelo PyMC, diagnostico, metricas, overfitting, residuos, efectos y persistencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3213d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 110,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"font.size\": 11,\n",
    "})\n",
    "PALETTE = {\"train\": \"#2196F3\", \"val\": \"#FF5722\", \"fit\": \"#4CAF50\"}\n",
    "\n",
    "print(\"✅ Librerias importadas\")\n",
    "print(f\"PyMC: {pm.__version__} | ArviZ: {az.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecdaf41",
   "metadata": {},
   "source": [
    "## 1 — Carga y EDA rapido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e42c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../data/raw/amazon_sales_dataset.csv\")\n",
    "df = pl.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(\"Columnas:\", df.columns)\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effab6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparacion de features\n",
    "df_pd = df.to_pandas()\n",
    "df_pd[\"order_date\"] = pd.to_datetime(df_pd[\"order_date\"])\n",
    "df_pd[\"is_weekend\"] = df_pd[\"order_date\"].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "\n",
    "FEATURES = [\"discount_percent\", \"rating\", \"is_weekend\"]\n",
    "TARGET = \"quantity_sold\"\n",
    "\n",
    "X = df_pd[FEATURES].copy()\n",
    "y = df_pd[TARGET].copy()\n",
    "\n",
    "y_bins = pd.qcut(y, q=5, labels=False, duplicates=\"drop\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y_bins\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train):,} | Val: {len(X_val):,}\")\n",
    "print(f\"Train mean/var: {y_train.mean():.2f}/{y_train.var():.2f}\")\n",
    "print(f\"Val mean/var: {y_val.mean():.2f}/{y_val.var():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf5088d",
   "metadata": {},
   "source": [
    "## 2 — Estandarizacion y modelo PyMC\n",
    "\n",
    "$$\\log(\\lambda_i) = \\alpha + \\beta_d \\cdot discount^* + \\beta_r \\cdot rating^* + \\beta_w \\cdot is\\_weekend$$\n",
    "\n",
    "$$y_i \\sim \\text{Poisson}(\\lambda_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e701206",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_mean = X_train[\"discount_percent\"].mean()\n",
    "discount_std = X_train[\"discount_percent\"].std()\n",
    "rating_mean = X_train[\"rating\"].mean()\n",
    "rating_std = X_train[\"rating\"].std()\n",
    "\n",
    "def standardize(X_df, d_mean, d_std, r_mean, r_std):\n",
    "    Xs = X_df.copy()\n",
    "    Xs[\"discount_scaled\"] = (Xs[\"discount_percent\"] - d_mean) / d_std\n",
    "    Xs[\"rating_scaled\"] = (Xs[\"rating\"] - r_mean) / r_std\n",
    "    return Xs\n",
    "\n",
    "X_train_s = standardize(X_train, discount_mean, discount_std, rating_mean, rating_std)\n",
    "X_val_s = standardize(X_val, discount_mean, discount_std, rating_mean, rating_std)\n",
    "\n",
    "d_train = X_train_s[\"discount_scaled\"].values.astype(float)\n",
    "r_train = X_train_s[\"rating_scaled\"].values.astype(float)\n",
    "w_train = X_train_s[\"is_weekend\"].values.astype(float)\n",
    "y_train_np = y_train.values.astype(int)\n",
    "\n",
    "d_val = X_val_s[\"discount_scaled\"].values.astype(float)\n",
    "r_val = X_val_s[\"rating_scaled\"].values.astype(float)\n",
    "w_val = X_val_s[\"is_weekend\"].values.astype(float)\n",
    "y_val_np = y_val.values.astype(int)\n",
    "\n",
    "with pm.Model() as poisson_model:\n",
    "    alpha = pm.Normal(\"alpha\", mu=np.log(y_train_np.mean()), sigma=1.0)\n",
    "    beta_d = pm.Normal(\"beta_d\", mu=0, sigma=0.5)\n",
    "    beta_r = pm.Normal(\"beta_r\", mu=0, sigma=0.5)\n",
    "    beta_w = pm.Normal(\"beta_w\", mu=0, sigma=0.3)\n",
    "\n",
    "    log_lambda = alpha + beta_d * d_train + beta_r * r_train + beta_w * w_train\n",
    "    pm.Poisson(\"y_obs\", mu=pm.math.exp(log_lambda), observed=y_train_np)\n",
    "\n",
    "    trace = pm.sample(\n",
    "        draws=DRAW_SAMPLES,\n",
    "        tune=TUNE_SAMPLES,\n",
    "        target_accept=TARGET_ACCEPT,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        progressbar=True,\n",
    "        return_inferencedata=True,\n",
    "    )\n",
    "\n",
    "print(\"✅ Muestreo completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e94a9d",
   "metadata": {},
   "source": [
    "## 3 — Diagnostico MCMC\n",
    "Verificar `r_hat <= 1.01` y `ess_bulk >= 400`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7442a146",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = az.summary(trace, var_names=[\"alpha\", \"beta_d\", \"beta_r\", \"beta_w\"], round_to=4)\n",
    "display(summary)\n",
    "\n",
    "rhat_max = summary[\"r_hat\"].max()\n",
    "ess_min = summary[\"ess_bulk\"].min()\n",
    "print(f\"R_hat max: {rhat_max:.4f}\")\n",
    "print(f\"ESS min: {ess_min:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a075230",
   "metadata": {},
   "source": [
    "## 4 — Metricas: MAE, RMSE y R2 Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12581dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "post = trace.posterior\n",
    "alpha_m = float(post[\"alpha\"].mean())\n",
    "beta_d_m = float(post[\"beta_d\"].mean())\n",
    "beta_r_m = float(post[\"beta_r\"].mean())\n",
    "beta_w_m = float(post[\"beta_w\"].mean())\n",
    "\n",
    "def predict_lambda(d, r, w):\n",
    "    return np.exp(alpha_m + beta_d_m * d + beta_r_m * r + beta_w_m * w)\n",
    "\n",
    "lam_train = predict_lambda(d_train, r_train, w_train)\n",
    "lam_val = predict_lambda(d_val, r_val, w_val)\n",
    "\n",
    "mae_train = mean_absolute_error(y_train_np, lam_train)\n",
    "mae_val = mean_absolute_error(y_val_np, lam_val)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train_np, lam_train))\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val_np, lam_val))\n",
    "\n",
    "def poisson_deviance(y, lam):\n",
    "    y = np.array(y, dtype=float)\n",
    "    lam = np.array(lam, dtype=float)\n",
    "    safe = np.where(y > 0, y * np.log(y / lam) - (y - lam), -(y - lam))\n",
    "    return 2 * safe.sum()\n",
    "\n",
    "lam_null_train = np.full_like(lam_train, y_train_np.mean())\n",
    "lam_null_val = np.full_like(lam_val, y_val_np.mean())\n",
    "\n",
    "dev_model_train = poisson_deviance(y_train_np, lam_train)\n",
    "dev_null_train = poisson_deviance(y_train_np, lam_null_train)\n",
    "dev_model_val = poisson_deviance(y_val_np, lam_val)\n",
    "dev_null_val = poisson_deviance(y_val_np, lam_null_val)\n",
    "\n",
    "r2_train = 1 - dev_model_train / dev_null_train\n",
    "r2_val = 1 - dev_model_val / dev_null_val\n",
    "\n",
    "print(f\"Train -> MAE {mae_train:.4f} | RMSE {rmse_train:.4f} | R2 {r2_train:.4f}\")\n",
    "print(f\"Val   -> MAE {mae_val:.4f} | RMSE {rmse_val:.4f} | R2 {r2_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eac538",
   "metadata": {},
   "source": [
    "## 5 — Overfitting, residuos y efectos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc2cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_rel = (mae_val - mae_train) / mae_train * 100\n",
    "status = \"✅ Generaliza correctamente\" if abs(delta_rel) < OVERFIT_THRESHOLD * 100 else \"⚠️ Posible overfitting\"\n",
    "print(f\"Delta relativo MAE: {delta_rel:+.2f}% -> {status}\")\n",
    "\n",
    "resid_train = (y_train_np - lam_train) / np.sqrt(lam_train)\n",
    "resid_val = (y_val_np - lam_val) / np.sqrt(lam_val)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "for ax, resid, lam, label, color in zip(\n",
    "    axes, [resid_train, resid_val], [lam_train, lam_val], [\"Train\", \"Validacion\"], [PALETTE[\"train\"], PALETTE[\"val\"]]\n",
    " ):\n",
    "    ax.scatter(lam, resid, alpha=0.25, s=8, color=color, label=label)\n",
    "    ax.axhline(0, color=\"black\", lw=1.2, ls=\"--\")\n",
    "    ax.set_xlabel(\"lambda estimada\")\n",
    "    ax.set_ylabel(\"Residuo de Pearson\")\n",
    "    ax.set_title(f\"Residuos - {label}\")\n",
    "    ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "delta_d_10 = 10 / discount_std\n",
    "factor_d = np.exp(beta_d_m * delta_d_10)\n",
    "delta_r_1 = 1 / rating_std\n",
    "factor_r = np.exp(beta_r_m * delta_r_1)\n",
    "factor_w = np.exp(beta_w_m)\n",
    "\n",
    "print(f\"+10 pp descuento -> x{factor_d:.4f} ({(factor_d-1)*100:+.2f}%)\")\n",
    "print(f\"+1 punto rating  -> x{factor_r:.4f} ({(factor_r-1)*100:+.2f}%)\")\n",
    "print(f\"Fin de semana    -> x{factor_w:.4f} ({(factor_w-1)*100:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18cfa98",
   "metadata": {},
   "source": [
    "## 6 — Persistencia de artefactos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5ab456",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = Path(\"../models/modelo4\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "trace_path = MODEL_DIR / \"modelo4_trace.nc\"\n",
    "trace.to_netcdf(str(trace_path))\n",
    "\n",
    "scaler_data = {\n",
    "    \"discount_mean\": discount_mean,\n",
    "    \"discount_std\": discount_std,\n",
    "    \"rating_mean\": rating_mean,\n",
    "    \"rating_std\": rating_std,\n",
    "    \"metrics\": {\n",
    "        \"mae_train\": mae_train,\n",
    "        \"mae_val\": mae_val,\n",
    "        \"rmse_train\": rmse_train,\n",
    "        \"rmse_val\": rmse_val,\n",
    "        \"r2_train\": r2_train,\n",
    "        \"r2_val\": r2_val,\n",
    "        \"delta_rel\": delta_rel,\n",
    "    },\n",
    "}\n",
    "\n",
    "scaler_path = MODEL_DIR / \"modelo4_scaler.joblib\"\n",
    "joblib.dump(scaler_data, scaler_path)\n",
    "\n",
    "print(f\"✅ Trace guardado en: {trace_path}\")\n",
    "print(f\"✅ Scaler/métricas guardado en: {scaler_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
