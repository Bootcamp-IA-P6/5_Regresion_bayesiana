


# üìä Proyecto de Modelado Bayesiano con PyMC
Este repositorio contiene una implementaci√≥n avanzada de modelos estad√≠sticos bajo el enfoque bayesiano, utilizando PyMC para el muestreo y ArviZ para el an√°lisis de diagn√≥sticos y visualizaci√≥n de resultados.

El proyecto abarca tres arquitecturas fundamentales:

Regresi√≥n Lineal Bayesiana: Para entender relaciones continuas.

Regresi√≥n Log√≠stica Bayesiana: Para problemas de clasificaci√≥n y probabilidades.

Modelo Jer√°rquico (Multinivel): Para capturar la variabilidad en diferentes niveles de agrupaci√≥n de los datos, permitiendo el "intercambio de informaci√≥n" entre grupos.

üöÄ Gu√≠a de Inicio R√°pido
Sigue estos pasos para replicar el entorno de desarrollo y ejecutar los modelos.

1. **Preparaci√≥n del Entorno**
Es fundamental aislar las dependencias para evitar conflictos de versiones.
###  Crear el entorno virtual
python -m venv venv

### Activar el entorno (Windows)
.\venv\Scripts\activate

### Activar el entorno (Linux/Mac)
source venv/bin/activate  

### 2. Instalaci√≥n de Dependencias
Utilizamos librer√≠as de alto rendimiento para el manejo de datos y computaci√≥n cient√≠fica:

- Polars: Para un procesamiento de datos ultra r√°pido (alternativa eficiente a Pandas).

- PyMC: Nuestro motor de inferencia bayesiana.

- ArviZ: Herramienta esencial para diagn√≥sticos de cadenas MCMC y visualizaci√≥n.

- Joblib: Para la persistencia de modelos y paralelizaci√≥n.

- pip install pymc arviz polars joblib matplotlib seaborn

## üõ†Ô∏è Flujo de Trabajo del Proyecto

El desarrollo se dividi√≥ en las siguientes fases t√©cnicas:Carga de Datos: Implementada con polars para garantizar eficiencia en la lectura y preprocesamiento.Definici√≥n del Prior: Selecci√≥n de distribuciones a priori (Normal, Half-Cauchy, etc.) basadas en conocimiento experto o criterios no informativos.Muestreo (Inferencia): Ejecuci√≥n del algoritmo NUTS (No-U-Turn Sampler) para obtener las distribuciones posteriores.Validaci√≥n: Uso de arviz para verificar la convergencia mediante el indicador $\hat{R}$ (R-hat) y el tama√±o efectivo de la muestra (ESS).Serializaci√≥n: Guardado de los trazos y modelos resultantes mediante joblib para su posterior uso sin necesidad de re-entrenar.




üìä Proyecto de Modelado Bayesiano con PyMC
Este repositorio contiene una implementaci√≥n avanzada de modelos estad√≠sticos bajo el enfoque bayesiano, utilizando PyMC para el muestreo y ArviZ para el an√°lisis de diagn√≥sticos y visualizaci√≥n de resultados.

El proyecto abarca tres arquitecturas fundamentales:

Regresi√≥n Lineal Bayesiana: Para entender relaciones continuas.

Regresi√≥n Log√≠stica Bayesiana: Para problemas de clasificaci√≥n y probabilidades.

Modelo Jer√°rquico (Multinivel): Para capturar la variabilidad en diferentes niveles de agrupaci√≥n de los datos, permitiendo el "intercambio de informaci√≥n" entre grupos.

üöÄ Gu√≠a de Inicio R√°pido


## Estructura de Carpetas 
![Estructura del modelo](https://github.com/Bootcamp-IA-P6/5_Regresion_bayesiana/blob/develop/img/Estructura.png?raw=true)



1. Preparaci√≥n del Entorno
Es fundamental aislar las dependencias para evitar conflictos de versiones.

Bash
# Crear el entorno virtual
python -m venv venv

# Activar el entorno (Windows)
.\venv\Scripts\activate

# Activar el entorno (Linux/Mac)
source venv/bin/activate
2. Instalaci√≥n de Dependencias
Utilizamos librer√≠as de alto rendimiento para el manejo de datos y computaci√≥n cient√≠fica:

Polars: Para un procesamiento de datos ultra r√°pido (alternativa eficiente a Pandas).

PyMC: Nuestro motor de inferencia bayesiana.

ArviZ: Herramienta esencial para diagn√≥sticos de cadenas MCMC y visualizaci√≥n.

Joblib: Para la persistencia de modelos y paralelizaci√≥n.

Bash
pip install pymc arviz polars joblib matplotlib seaborn
üõ†Ô∏è Flujo de Trabajo del Proyecto
El desarrollo se dividi√≥ en las siguientes fases t√©cnicas:

Carga de Datos: Implementada con polars para garantizar eficiencia en la lectura y preprocesamiento.

Definici√≥n del Prior: Selecci√≥n de distribuciones a priori (Normal, Half-Cauchy, etc.) basadas en conocimiento experto o criterios no informativos.

Muestreo (Inferencia): Ejecuci√≥n del algoritmo NUTS (No-U-Turn Sampler) para obtener las distribuciones posteriores.

Validaci√≥n: Uso de arviz para verificar la convergencia mediante el indicador  
R
^
  (R-hat) y el tama√±o efectivo de la muestra (ESS).

Serializaci√≥n: Guardado de los trazos y modelos resultantes mediante joblib para su posterior uso sin necesidad de re-entrenar.

### üìà Resumen de Modelos
Modelo	Uso Principal	Caracter√≠sticas
Lineal	Predicci√≥n de valores continuos.	Relaci√≥n directa entre variables independientes y dependientes.
Log√≠stico	Clasificaci√≥n binaria.	Uso de funci√≥n de enlace logit para modelar probabilidades.
Jer√°rquico	Datos agrupados o anidados.	Estima par√°metros globales y locales simult√°neamente, ideal para datos con estructura de grupos.


## Modelo Bayesiano Lineal 

![Modelo Bayesiano Lineal 1](https://github.com/Bootcamp-IA-P6/5_Regresion_bayesiana/blob/develop/img/ModeloLinealBayesiano1.png?raw=true)

üìà Regresi√≥n Lineal Bayesiana: Interpretaci√≥n de Resultados
Un Modelo Bayesiano Lineal estima la relaci√≥n entre una variable dependiente (Ingreso Total) y una independiente (Precio Descontado) utilizando distribuciones de probabilidad. A diferencia de la regresi√≥n tradicional que te da una sola l√≠nea "fija", aqu√≠ obtenemos todo un rango de posibilidades que cuantifican nuestra incertidumbre.


L√≠nea Roja (Media de la Regresi√≥n): Representa el valor m√°s probable de la relaci√≥n. Indica que a medida que el precio descontado (estandarizado) aumenta, el ingreso total tiende a subir siguiendo esta trayectoria central.

Haces de L√≠neas Azules (Muestras de la Posterior): Cada l√≠nea azul es una hip√≥tesis v√°lida generada por el modelo. Al haber muchas l√≠neas cerca de la roja, confirmamos que el modelo tiene una direcci√≥n clara, aunque la dispersi√≥n en los valores altos muestra d√≥nde hay mayor incertidumbre.

Intervalo de Credibilidad (94%): El sombreado gris (HDI) define el rango donde, con un 94% de certeza, se encuentra la verdadera relaci√≥n. Es la herramienta clave para la toma de decisiones basada en riesgos.

Estandarizaci√≥n: El eje X est√° estandarizado (centrado en 0), lo que facilita que el algoritmo de PyMC converja m√°s r√°pido y que el intercepto sea m√°s f√°cil de interpretar.


## Modelo Bayesiano Logistico 
![Modelo Bayesiano Logistico 2 ]([img/ModeloLogistico2.(https://github.com/Bootcamp-IA-P6/5_Regresion_bayesiana/blob/develop/img/modeloLogistico2.png?raw=true))

Eje Y - P(Best Seller): Representa la probabilidad de ser un "S√∫per Ventas". El valor var√≠a de 0 a 1 (0% a 100%).

L√≠nea Azul Central: Es la media posterior. Nos indica la tendencia promedio. Curiosamente, en tu gr√°fico la l√≠nea es casi plana cerca del 0.5 (50%), lo que sugiere que, para este modelo en particular, el rating por s√≠ solo no es un predictor extremadamente fuerte para cambiar la probabilidad de ser Best Seller.

√Årea Sombreada Azul (Incertidumbre 5-95%): Este es el Intervalo de Credibilidad. Es la parte m√°s importante del an√°lisis bayesiano:

Incertidumbre Alta: Al ser un √°rea muy ancha (que va desde casi 0.1 hasta 0.9), el modelo nos est√° diciendo: "No tengo datos suficientes o el rating es muy ruidoso para asegurar si un producto ser√° Best Seller".

Si tuvi√©ramos miles de datos muy claros, esa banda sombreada ser√≠a muy delgadita alrededor de la l√≠nea central.

A diferencia de los modelos tradicionales, el uso de PyMC nos permite visualizar no solo la probabilidad media, sino el grado de incertidumbre (Intervalo de Credibilidad del 90%). En la gr√°fica se observa que el modelo mantiene una postura cautelosa debido a la dispersi√≥n de los datos, lo cual es vital para evitar decisiones basadas en falsas certezas.


## Modelo Jerarquico Bayesiano 
![Modelo Bayesiano Jerarquico 3 ](https://github.com/Bootcamp-IA-P6/5_Regresion_bayesiana/blob/develop/img/ModeloJerarquico3.png?raw=true))

üèõÔ∏è Modelo Bayesiano Jer√°rquico (Multinivel)
El objetivo de este modelo es capturar la estructura anidada de los datos. En lugar de asumir que todas las categor√≠as se comportan igual, permitimos que cada una tenga su propio intercepto, pero compartiendo una distribuci√≥n com√∫n ("hiperprior").

Beneficios clave:

Intercambio de informaci√≥n: Los grupos con mucha informaci√≥n ayudan a estabilizar las estimaciones de los grupos con pocos datos.

Robustez: Reduce el riesgo de sobreajuste en categor√≠as peque√±as.

An√°lisis Comparativo: Como se observa en el gr√°fico de intervalos (HDI), podemos comparar directamente si las diferencias entre categor√≠as (p. ej., a_cat[1] vs a_cat[3]) son estad√≠sticamente significativas si sus intervalos no se solapan.





